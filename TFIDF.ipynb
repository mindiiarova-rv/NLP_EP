{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIyjj8TskT_"
      },
      "source": [
        "<center>\n",
        "<h1><br\\></h1>\n",
        "<h1>INF582: INTRODUCTION TO TEXT MINING AND NLP</h1>\n",
        "</center>\n",
        "<center>\n",
        "<h2>Lab Session 1: TF-IDF</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Dr Guokan Shang and Dr. Hadi Abdine <br> contact: hadi.abdine@polytechnique.edu</h4>\n",
        "<h5>Friday, January 10, 2025</h5>\n",
        "<h4><b>Student Name:</b> </h4>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit to Moodle a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing a your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>January 17, 2025 08:29 AM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "<h3><b>1. Introduction</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Documents are traditionally represented with the vector space model, also known as the bag-of-words representation [<a href='https://nlp.stanford.edu/IR-book/' >Manning et al.</a>]. With this approach, each document di from the collection D = {d1 . . . dm } of size m is associated with an n-dimensional feature vector, where n is the number of unique terms in the preprocessed collection. The set of unique terms T = {t1 . . . tn } is called the vocabulary.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF), is a method used to compute the coordinates of a document in the vector space.\n",
        "</p>\n",
        "\n",
        "<h3><b>2. Learning Objective</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this lab, you will learn how to compute the TF-IDF representation and use it in 3 different tasks:\n",
        "<ul>\n",
        "<li>computing the cosine similarity between documents</li>\n",
        "<li>executing a query against a collection of documents using the inverted index,</li>\n",
        "<li>performing supervised document classification.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<h3><b>3. Computing TF-IDF</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "The assumption is that the importance of a word to a document increases when its frequency increases. However, considering the frequency as the only factor to judge the importance of a word would result in giving greater weight to commonly used terms such as stopwords, which could be misleading in many tasks. TF-IDF mitigates this problem by introducing a factor that diminishes the weight of words that occur frequently in other documents in the same collection. The TF-IDF weight computation is based on the product of two separate factors, namely the Term Frequency (TF) and the Inverse Document Frequency (IDF). More specifically:\n",
        "\n",
        "$$ tf\\text{-}idf(t,d,\\mathcal{D}) = tf(t,d) \\times idf(t, \\mathcal{D}) $$\n",
        "\n",
        "There are many ways to determine tf and idf . In our case, $tf (t, d)$ is the number of times term $t$ appears in document $d$, and $idf (t, D) = ln (\\frac{m}{1+df (t)}) + 1$, with $df (t)$ the number of documents in the collection $D$ that contains $t$. We can notice that the weight of a term in a document increases when its frequency increases in this document (first factor), and decreases when the number of documents in the collection containing this term increases (second factor). M ∈ $R^{m×n}$ is the TF-IDF matrix of $D$, where $M_{ij}$ is the TF-IDF weight of the $jth$ word in the $ith$ document.\n",
        "\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU8DZ-kVPuBX"
      },
      "source": [
        "### Importing libraries and downloading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcO2sbrulEJ",
        "outputId": "8624a568-9b1d-47a0-c4cc-6aa0695c4ddc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('webkb-test-stemmed.txt', <http.client.HTTPMessage at 0x7c65451aab00>)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152573&authkey=AFz5kPjESCHRl3s\", 'webkb-train-stemmed.txt')\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152576&authkey=ACypGA77xWokzQ8\", 'webkb-test-stemmed.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9_vD4yCkGv"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "The next cell implements a <i>tfidf</i> class. When initialized, a <i>tfidf</i> object takes as input a collection of documents and computes the <i>IDF</i> of words in this collection. You can provide a list of stopwords to be ignored. Two methods are defined in this class: <i>tf</i> that returns the frequency of unique words in a document and transform that returns the <i>TF-IDF</i> matrix of the collection. Fill in the gaps in the different functions of <i>tfidf</i> class.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kaN3aNU_svtv"
      },
      "outputs": [],
      "source": [
        "documents = [\"euler is the father of graph theory\",\n",
        "             \"graph theory studies the properties of graphs\",\n",
        "             \"bioinformatics studies the application of efficient algorithms in the biological field\"]\n",
        "\n",
        "class tfidf(object):\n",
        "    def __init__(self, collection, stop_words=[]):\n",
        "        '''collection is a list of strings'''\n",
        "        self.word2ind = {} # map each word in the collection to a unique index\n",
        "        self.ind2word = {}\n",
        "        self.word2idf = {} # map each word to its idf\n",
        "        self.collection = collection\n",
        "        self.documents = [document.split() for document in collection]\n",
        "\n",
        "\n",
        "        self.unique_words = list(set(word for document in self.documents for word in document)) # fill it, list of unique words in the collection\n",
        "        self.unique_words = [word for word in self.unique_words if word not in stop_words] # remove stopwords\n",
        "        self.count_unique_words = len(self.unique_words)\n",
        "        self.word2ind = dict(zip(self.unique_words,range(self.count_unique_words)))\n",
        "        self.ind2word = {v:k for k,v in self.word2ind.items()}\n",
        "        self.count_documents = len(collection)\n",
        "\n",
        "        # compute the idf of unqiue words in the collection\n",
        "        for word in self.word2ind.keys():\n",
        "            count = 0\n",
        "            for document in self.documents:\n",
        "                if word in document:\n",
        "                    count += 1\n",
        "            idf = np.log(self.count_documents / (count + 1)) + 1\n",
        "            self.word2idf[word] = idf\n",
        "\n",
        "    def getWordFromInd(self, ind):\n",
        "        return self.ind2word[ind]\n",
        "\n",
        "    def getListWords(self):\n",
        "        return self.unique_words\n",
        "\n",
        "    def tf(self, document):\n",
        "        '''\n",
        "        return the frequency of each unique word in document.\n",
        "        document is a list of strings\n",
        "        '''\n",
        "        word2frequency = {}\n",
        "        for word in document:\n",
        "            word2frequency[word] = word2frequency.get(word, 0) + 1 # increment, creating key if it doesn't already exist\n",
        "        return word2frequency\n",
        "\n",
        "    def transform(self, collection):\n",
        "        documents = [document.split() for document in collection] # tokenize documents in the collection\n",
        "        tfidf_mat = np.zeros(shape=(len(collection), self.count_unique_words)) # fill it, intialize tfidf matrix with zeros\n",
        "        # compute tfidf\n",
        "        for ind, document in enumerate(documents):\n",
        "            word2frequency = self.tf(document)\n",
        "            for word in word2frequency.keys():\n",
        "                if word in self.word2ind:\n",
        "                    tfidf_mat[ind, self.word2ind[word]] = word2frequency[word] * self.word2idf[word] # fill it, tfidf\n",
        "        return tfidf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHf1bwk6GGn0"
      },
      "source": [
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1 (5 points): </b><br>\n",
        "In the above script we provide a small collection documents. In order to validate your functions, compute manually the TF-IDF of the word <b>’the’</b> in the last document and check if you get the same result as with your code.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvu2ISM-uckV",
        "outputId": "b59ff6b9-341f-4ceb-fcf7-d3b6d6326670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4246358550964382\n"
          ]
        }
      ],
      "source": [
        "from operator import index\n",
        "#tfidf of 'the' in last document\n",
        "# compute the tf-idf of the word 'the' using the code\n",
        "\n",
        "tfidf_mat = tfidf(documents).transform(documents)\n",
        "index = tfidf(documents).word2ind['the']\n",
        "print(tfidf_mat[-1, index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2prjyXnWOM-w"
      },
      "source": [
        "The number of times term $t$ appears in document $d = 2$,$df = 3$, $m = 3$\n",
        "\n",
        "$\\text{TF-IDF} = 2 \\cdot \\ln\\left(\\frac{3}{4}\\right) + 1 = 1.4$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfnpxmskGtfI"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "1.42\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u2-8Oc0HQcj"
      },
      "source": [
        "<h3><b>4. Cosine Similarity</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will compute the cosine similarity of two documents using the TF-IDF representation. The similarity concept is crucial in search engines as well as in text mining applications.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathrm{similarity}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\times \\|v_2\\|}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Normally, the cosine similarity ranges from -1 to +1. However, in our case all the entries of the TF-IDF features are positive, thus it ranges from 0 to 1. The cosine similarity matrix is a square matrix representing the similarity between all pairs of documents in a collection. In other words, if S is the similarity matrix, then $S_{ij} = similarity(v_i , v_j)$ where $v_i$ is the TF-IDF vector of the ith document and $v_j$ is the TF-IDF vector of the jth document.\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDPJQZcTIVbx"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Complete the two functions $\\texttt{cosine_similarity()}$ and $\\texttt{cosine_similarity_matrix()}$ in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OXKsoLZ3Gq3a"
      },
      "outputs": [],
      "source": [
        "documents = [\"i like that music\",\n",
        "\t         \"this song appeals to me\",\n",
        "\t         \"i like graph theory\",\n",
        "\t         \"euler is the father of graph theory\",\n",
        "           \"graph theory studies the properties of graphs\",\n",
        "           \"the quick brown fox jumps over the lazy dog\",\n",
        "           \"the quick fox jumps over the brown lazy dog\"]\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    '''returns the cosine similarity of 2 vectors'''\n",
        "    numerator = np.dot(v1, v2)\n",
        "    denominator = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    return numerator / denominator\n",
        "\n",
        "def cosine_similarity_matrix(mat):\n",
        "    '''\n",
        "    returns the cosine similarity matrix\n",
        "    the ith row in mat represents the ith vector\n",
        "    '''\n",
        "    similarity_matrix = np.zeros(shape=(mat.shape[0], mat.shape[0])) # fill it\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[0]):\n",
        "            similarity_matrix[i][j] = cosine_similarity(mat[i], mat[j]) # fill it\n",
        "    return similarity_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBvKViDZJSLN"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2 (2 points): </b><br>\n",
        "Run the code of the next cell and examine its output. What do you observe about the similarity matrix? What can we do to speed-up the computation?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048fxiP_JSTk",
        "outputId": "f536ca8a-8ac7-43b5-879e-2893e13dcd28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.4845028  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.4845028  0.         1.         0.28294469 0.28294469 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.28294469 1.         0.39794976 0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.28294469 0.39794976 1.         0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "t = tfidf(documents)\n",
        "tfidf_matrix = t.transform(documents)\n",
        "similarity_matrix = cosine_similarity_matrix(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilpinMDXTLs_",
        "outputId": "3baf48f1-0483-407e-a5cd-21c817b5a261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8472978603872037\n"
          ]
        }
      ],
      "source": [
        "index = tfidf(documents).word2ind['like']\n",
        "print(tfidf_matrix[0, index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJmYOClPUSk9",
        "outputId": "c33f9d65-df78-49b4-9d9b-219a01e8820a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['brown', 'lazy', 'quick', 'to', 'studies', 'music', 'jumps', 'over', 'appeals', 'me', 'like', 'graphs', 'this', 'the', 'is', 'of', 'graph', 'euler', 'song', 'dog', 'that', 'properties', 'father', 'fox', 'theory', 'i'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf(documents).word2ind.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arwIoXLBT0nO",
        "outputId": "0c4241bf-a0b1-4e91-bb95-1658a96bec52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       2.25276297, 0.        , 0.        , 0.        , 0.        ,\n",
              "       1.84729786, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       2.25276297, 0.        , 0.        , 0.        , 0.        ,\n",
              "       1.84729786])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_matrix[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXvR5qzSTndN",
        "outputId": "33f63502-8535-4f95-9bfe-13de371525e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.252762968495368\n"
          ]
        }
      ],
      "source": [
        "index = tfidf(documents).word2ind['appeals']\n",
        "print(tfidf_matrix[1, index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBm-xhaJgoD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "\n",
        "**What do you observe about the similarity matrix? What can we do to speed-up the computation?**\n",
        "\n",
        "The matrix is symmetric, which means that the computation for one half of the matrix (above or below the diagonal) is redundant.\n",
        "The diagonal elements of the matrix are all 1, as the cosine similarity of a vector with itself is always 1.\n",
        "Many values are close to or equal to 0, indicating a high degree of dissimilarity between some documents.\n",
        "\n",
        "**What can we do to speed-up the computation?**\n",
        "Since the matrix is symmetric, we only need to compute values for one triangle.\n",
        "Use sparse matrix\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRJaBc2M6Rp"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3 (3 points): </b><br>\n",
        "Notice the similarity between first two documents and between last two documents and state two drawbacks of the bag-of-words representation.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1kapOTM_qD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "\n",
        "The similarity score between the first document and the second document is 0. This indicates that there are no common terms between these two documents, despite both discussing preferences for music or songs.\n",
        "This is a drawback of the bag-of-words (BoW) representation as it cannot capture semantic or contextual similarity\n",
        "\n",
        "The similarity score between the 6th document and the 7th document is 1, showing they are identical in terms of BoW representation. However, the two documents have slightly different word orders, which BoW does not account for.\n",
        "\n",
        "\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7QybieKBht"
      },
      "source": [
        "<h3><b>5. Inverted Index</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Typically, search engines execute queries against a huge document collection.\n",
        "To execute a query we can simply calculate a similarity measure between the query and all the documents in the collection.<br>\n",
        "However, doing so is very inefficient. Instead, it is possible to eliminate a lot of documents from the candidate set by using the inverted index.\n",
        "The inverted index consists of a mapping from words to documents.\n",
        "Each unique word in the vocabulary is mapped to a list containing the documents in which the word appears and the position of the word in these documents. Stemming, which is reducing tokens to a root form, could be useful in this task. For example a stemming algorithm would reduce <i>computing</i>, <i>computers</i> and <i>computation</i> to <i>comput</i> and thus identifying them as one unique token instead of three different ones. In our code we consider two dictionaries to perform the mapping. In one of the dictionaries the words are stemmed, and in the other they are not.\n",
        "Instead of ranking all the documents in the collection, the inverted index allows us to rank only the relevant documents.\n",
        "\n",
        "In our code we provide several functions:\n",
        "<ul>\n",
        "<li><i>at_least_one_unigram()</i>, returns documents containing at least one query word.\n",
        "<li> <i>all_unigrams()</i>, returns documents containing all query words.\n",
        "<li> <i>ngrams()</i>, returns documents containing all query words in the same order as in the query.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y4oTerTwuz9",
        "outputId": "dacf63e5-e7f9-49b2-a4f1-8060c09a68a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCedyrCjwn1B",
        "outputId": "5b653920-3f0a-4b86-ec81-1207f4bfae31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CNn06vxA95",
        "outputId": "d677d8c6-6d46-48f2-9e2c-76769fa005c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o-fHSDc0JhvQ"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "def clean_tokenize(doc):\n",
        "    words = word_tokenize(doc.lower())\n",
        "    return words\n",
        "\n",
        "def index_one_doc(doc,to_stem):\n",
        "    '''\n",
        "    creates dict (tok,positions) for each tok in the document (as term list)\n",
        "    '''\n",
        "    tokpos = dict()\n",
        "    for t_idx,tok in enumerate(doc):\n",
        "        if to_stem:\n",
        "           tok = stemmer.stem(tok)\n",
        "        if tok in tokpos:\n",
        "            tokpos[tok].append(t_idx)\n",
        "        else:\n",
        "            tokpos[tok] = [t_idx]\n",
        "    return tokpos\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "docs = ['The quick brown fox jumps over the lazy dog',\n",
        "        'The brown quick fox jumps over the lazy dog',\n",
        "        'Luke is the mechanical and electrical engineer of the new group',\n",
        "        'Instead of buying a new engine, buy a new car',\n",
        "        'An engineer may design car engines of all sorts',\n",
        "        'Engineers use logic, but not necessarily imagination',\n",
        "        'Logic will take you from A to Z, imagination will take you everywhere.',\n",
        "        'Continuous effort, not strength or intelligence, is the key to \\\n",
        "        unlocking our potential. And curiosity.',\n",
        "        'It’s OK to have your eggs in one basket as long as you control what \\\n",
        "        happens to that basket.'\n",
        "        ]\n",
        "\n",
        "cleaned_docs = []\n",
        "for doc in docs:\n",
        "    to_app = clean_tokenize(doc)\n",
        "    cleaned_docs.append(to_app)\n",
        "\n",
        "# = = = = = = = = = = = = = = =\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=True)\n",
        "\n",
        "index_one_doc(cleaned_docs[3],to_stem=False)\n",
        "\n",
        "'''\n",
        "- queries are not case sensitive\n",
        "- we are indexing punctuation marks\n",
        "- we index stopwords and should keep stopwords in the queries (gives more expressivity)\n",
        "'''\n",
        "\n",
        "inverted_index = dict()\n",
        "inverted_index_stem = dict()\n",
        "\n",
        "for d_idx,doc in enumerate(cleaned_docs):\n",
        "\n",
        "    poslists_s = index_one_doc(doc,to_stem=True) # get positions of each token in the doc\n",
        "    for tok,poslist_s in poslists_s.items():\n",
        "        if tok in inverted_index_stem:\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # update\n",
        "        else:\n",
        "            inverted_index_stem[tok] = dict()\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s # initialize\n",
        "\n",
        "    poslists = index_one_doc(doc,to_stem=False)\n",
        "    for tok, poslist in poslists.items():\n",
        "        if tok in inverted_index:\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "        else:\n",
        "            inverted_index[tok] = dict()\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = = = = = = =\n",
        "\n",
        "def at_least_one_unigram(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *at least one* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.extend(list(inverted_index[unigram].keys()))\n",
        "    return list(set(to_return))\n",
        "\n",
        "def all_unigrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing *all* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.append(set(list(inverted_index[unigram].keys())))\n",
        "        else:\n",
        "            to_return.append(set())\n",
        "            break\n",
        "    to_return = to_return[0].intersection(*to_return)\n",
        "    return list(to_return)\n",
        "\n",
        "def ngrams(query,inverted_index):\n",
        "    '''\n",
        "    returns the indexes of the docs containing all unigrams in same order as the query\n",
        "    the query is a list of unigrams\n",
        "    '''\n",
        "    candidate_docs = all_unigrams(query,inverted_index)\n",
        "\n",
        "    to_return = []\n",
        "    for doc in candidate_docs:\n",
        "        poslists = []\n",
        "        for unigram in query:\n",
        "            to_append = inverted_index[unigram][doc]\n",
        "            if isinstance(to_append, int):\n",
        "                poslists.append([to_append])\n",
        "            else:\n",
        "                poslists.append(to_append)\n",
        "        # test whether the query words are consecutive\n",
        "        poslists_sub = [[elt-idx for elt in poslist] for idx,poslist in enumerate(poslists)]\n",
        "        if set(poslists_sub[0]).intersection(*poslists_sub):\n",
        "            to_return.append(doc)\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSfqenBLte7"
      },
      "source": [
        "## Queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5acI7y3yVqnH",
        "outputId": "da8fcfb0-8d85-43d4-d49e-d956f4a1e3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: engine car\n",
            "-----------------------------------------------\n",
            "docs containing at least one word in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing all the words in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "\n",
            "docs (stemmed) containing at least one word in the query (stemmed):\n",
            "* Luke is the mechanical and electrical engineer of the new group\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "* Engineers use logic, but not necessarily imagination\n",
            "\n",
            "docs (stemmed) containing all the words in the query (stemmed):\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing engine all the words in the query in the same order:\n",
            "********************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = ['engine','car']\n",
        "print('Query: {}'.format(' '.join(query)))\n",
        "print('-----------------------------------------------')\n",
        "\n",
        "docs_index = at_least_one_unigram(query,inverted_index)\n",
        "print('docs containing at least one word in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query,inverted_index)\n",
        "print('docs containing all the words in the query:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "query_stemmed = [stemmer.stem(elt) for elt in query]\n",
        "docs_index = at_least_one_unigram(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing at least one word in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query_stemmed,inverted_index_stem)\n",
        "print('docs (stemmed) containing all the words in the query (stemmed):')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = ngrams(query,inverted_index)\n",
        "print('docs containing engine all the words in the query in the same order:')\n",
        "for el in docs_index:\n",
        "    print('* {}'.format(docs[el]))\n",
        "\n",
        "print('********************************************************')\n",
        "print()\n",
        "\n",
        "tf_idf = tfidf(docs)\n",
        "query = ['new', 'car']\n",
        "tf_idf_query = tf_idf.transform([(' ').join(query)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvlLzsRtLRla"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqx4g0guLvED",
        "outputId": "59c51166-46f3-4da8-fd94-84a9a2f6986b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (new car) over all documents\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.67901611328125 ms\n",
            "\n",
            "Query (new car) over candidates containing at least one of the words\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.4379749298095703 ms\n"
          ]
        }
      ],
      "source": [
        "############ query over all documents\n",
        "print('Query ({}) over all documents'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "tf_idf_collection = tf_idf.transform(docs)   # fill it\n",
        "query_vector = tf_idf.transform([' '.join(query)])\n",
        "\n",
        "scores = cosine_similarity(query_vector, tf_idf_collection.T).flatten() # fill it, list containing the cosine similarities of the query against all documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))\n",
        "\n",
        "print('')\n",
        "\n",
        "############ query over candidate documents using inverted index\n",
        "print('Query ({}) over candidates containing at least one of the words'.format((' ').join(query)))\n",
        "print('-----------------------------------------------')\n",
        "time1 = time.time()\n",
        "candidate_docs_index = at_least_one_unigram(query, inverted_index) # fill it\n",
        "candidate_docs = [docs[el] for el in candidate_docs_index]\n",
        "tf_idf_collection = tf_idf.transform(candidate_docs)\n",
        "scores = cosine_similarity(query_vector, tf_idf_collection.T).flatten() # fill it, list containing the cosine similarities of the query against candidate documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(candidate_docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1)*1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z8vwmdeM0jv"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 4 (2.5 points): </b><br>\n",
        "Examine and interpret the output of the previous cell.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYhAq72iNMAc"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "\n",
        "\n",
        "As we can see, the obtained results are the same, but the computational time differs significantly, with the second method being approximately three times faster.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYNxB_yNMH9"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 5 (2.5 points): </b><br>\n",
        "If we execute a query the naive way, and then we execute it against the documents containing all the words in the query, is it possible to get different results? Motivate your answer\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huKEjZ6NMXr"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "\n",
        "The two methods can lead to different results because the naive approach considers partial matches, while the second method strictly requires all query terms to be present in the documents. This difference in filtering criteria affects the set of documents being ranked and, consequently, the final results.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R959CcOlNmfn"
      },
      "source": [
        "<h3><b>6. Supervised Classification</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will use the TF-IDF features to perform a supervised classification task. We will work with the WebKB dataset. It features academic webpages belonging to four different categories: (1) project, (2) course, (3) faculty, and (4) students, and contains 2,803 documents for training and 1,396 for testing. Documents have already been preprocessed with stopword removal and Porter stemming. The code that you will work with implements the following steps:\n",
        "\n",
        "<ul>\n",
        "<li>data loading,\n",
        "<li>computation of TF-IDF features for the training set,\n",
        "<li> computation of features for the test set. Note that the documents in the test set are represented in the space made of the unique terms in the training set only (words in the testing set absent from the training set are disregarded).\n",
        "<li>classifier training/testing. Naive Bayes classifier [<a href='https://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf'>McCallum and Nigam, 1998</a>], Multinomial Logistic Regression [<a href='https://www.learningtheory.org/colt2000/papers/CollinsSchapireSinger.pdf'>Collins et al., 2002</a>], Ran- dom Forest Classifier [1] and linear kernel SVM [<a href='https://link.springer.com/article/10.1007/BF00994018'>Cortes and Vapnik 1995</a>, <a href='https://www.researchgate.net/publication/28351286_Text_Categorization_with_Support_Vector_Machines'>Joachims 1998</a>] are compared,\n",
        "<li>get the most/least important words per class.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjbbZSmPVpu"
      },
      "source": [
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Complete the code in the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaDtszVDNLg8",
        "outputId": "e9ad0f41-94ea-4e32-eea1-41e323935617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logisitc Regression accuracy: 0.8839541547277937\n",
            "Random forest accuracy: 0.8567335243553008\n",
            "Naive Bayes accuracy: 0.8173352435530086\n",
            "Linear SVM accuracy: 0.8474212034383954\n",
            "0.2679083094555874\n",
            "\n",
            "Top 10\n",
            "project: laboratori faculti perform project peopl high lab hybrid request group\n",
            "student: address graduat wisc interest home zhang advisor work construct resum\n",
            "course: cse structur grade materi data assign comp instructor syllabu fall\n",
            "faculty: ufl cours perman recent cpsc associ teach henri fax professor\n",
            "Bottom 10\n",
            "project: interest address home fax fall offic email professor scienc\n",
            "student: professor perman henri faculti group hybrid fall fax comp\n",
            "course: research cours interest vision address cpsc person pictur berkelei\n",
            "faculty: graduat lab student advisor homework construct move syllabu riversid\n"
          ]
        }
      ],
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    # coef stores the weights of each feature (in unique term), for each class\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in top10)))\n",
        "\n",
        "def print_bot10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the lowest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        bot10 = np.argsort(clf.coef_[i])[0:9]\n",
        "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in bot10)))\n",
        "\n",
        "def prepare_data(path):\n",
        "    with open(path, 'r') as f:\n",
        "        s = f.read()\n",
        "    examples = s.split('\\n') # split to samples\n",
        "    examples = examples[:-1] # last element is empty\n",
        "    documents = []\n",
        "    labels = []\n",
        "    for el in examples:\n",
        "        example = el.split('\\t') # separate document from label\n",
        "        documents.append(example[1])\n",
        "        labels.append(example[0])\n",
        "    return documents, labels\n",
        "\n",
        "path_train = './webkb-train-stemmed.txt' # path to train data\n",
        "path_test = './webkb-test-stemmed.txt' # path to test data\n",
        "train_documents, train_labels = prepare_data(path_train)\n",
        "test_documents, test_labels = prepare_data(path_test)\n",
        "\n",
        "categories = set(train_labels) # get unique categoris\n",
        "category2ind = dict(zip(categories,range(len(categories)))) # map each category to index\n",
        "ind2category = {v:k for k,v in category2ind.items()} # map index to category\n",
        "\n",
        "train_labels_index = [category2ind[cat] for cat in train_labels] # replace labels by their indexes\n",
        "test_labels_index = [category2ind[cat] for cat in test_labels] # replace labels by their indexes\n",
        "\n",
        "t = tfidf(train_documents) # fill it, tfidf object\n",
        "tfidf_train = t.transform(train_documents)\n",
        "# fill it, get the tfidf training matrix\n",
        "tfidf_test = t.transform(test_documents) # fill it, get the tfidf text matrix\n",
        "\n",
        "lr = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=200)\n",
        "rf = RandomForestClassifier(n_estimators=20)\n",
        "nb = MultinomialNB()\n",
        "svm = LinearSVC(max_iter=2000)\n",
        "\n",
        "Classifiers = {'Logisitc Regression': lr, 'Random forest': rf, 'Naive Bayes': nb, 'Linear SVM': svm}\n",
        "\n",
        "for classifier in Classifiers.keys():\n",
        "    Classifiers[classifier].fit(tfidf_train, train_labels_index) # train each classifier\n",
        "    predicted = Classifiers[classifier].predict(tfidf_test) # perform prediction\n",
        "    accuracy = np.mean(predicted == np.array(test_labels_index)) # fill it, compute accuracy\n",
        "    print('{} accuracy: {}'.format(classifier, accuracy))\n",
        "\n",
        "predicted = np.zeros((len(test_labels_index))) + 3\n",
        "print(np.mean(predicted == np.array(test_labels_index)))\n",
        "print('')\n",
        "\n",
        "# choose one classfier and print top 10 and bottom 10 words per class\n",
        "feature_names = t.getListWords() # fill it\n",
        "classifier = Classifiers['Logisitc Regression'] # fill it\n",
        "print('Top 10')\n",
        "print_top10(feature_names, classifier, categories)\n",
        "print('Bottom 10')\n",
        "print_bot10(feature_names, classifier, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjXDedgQFxn"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 6 (5 points): </b><br>\n",
        "Examine the accuracy of different classifiers, examine the most/least important words and comment the results.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKQ-IEKTQUzD"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "Naive Bayes (81.73%) is slightly less accurate. This is expected for Naive Bayes in multi-class problems where feature dependencies are not well captured.\n",
        "\n",
        "Logistic Regression achieves the highest accuracy (88.39%). Logistic Regression benefits from its ability to handle linear decision boundaries and sparse features effectively.\n",
        "\n",
        "Based on the top 10 words for each class, we see that the words distinctly reflect the specific themes of their respective classes. The bottom 10 words for each class are less suitable for classification as they are either too generic or lack specific relevance to the class themes\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10 (main, Jan 15 2022, 11:40:53) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
